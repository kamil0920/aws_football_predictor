{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "CODE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(f\"./{CODE_FOLDER}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:42.640768100Z",
     "start_time": "2024-04-16T16:55:42.600079100Z"
    }
   },
   "id": "dcd6df911098ecb8",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !aws s3api create-bucket --bucket football-data-kamil --create-bucket-configuration LocationConstraint=eu-north-1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:42.685087700Z",
     "start_time": "2024-04-16T16:55:42.637766600Z"
    }
   },
   "id": "15812d9d5c4c5062",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import ipytest\n",
    "from pathlib import Path\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "CODE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "INFERENCE_CODE_FOLDER = CODE_FOLDER / \"inference\"\n",
    "INFERENCE_CODE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\", f\"./{INFERENCE_CODE_FOLDER}\"])\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "\n",
    "# By default, The SageMaker SDK logs events related to the default\n",
    "# configuration using the INFO level. To prevent these from spoiling\n",
    "# the output of this notebook cells, we can change the logging\n",
    "# level to ERROR instead.\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:42.804481800Z",
     "start_time": "2024-04-16T16:55:42.670087Z"
    }
   },
   "id": "6beb300bf7e34dc7",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "\n",
    "# Update this variable to your bucket name. This name must be unique\n",
    "# across all AWS accounts.\n",
    "BUCKET = os.environ[\"BUCKET\"]\n",
    "S3_LOCATION = f\"s3://{BUCKET}/football\"\n",
    "\n",
    "# To run this notebook in Local Model, this constant must be set to True.\n",
    "# I'm trying to do this automatically by checking for a specific environment\n",
    "# variable that is set by SageMaker when you run the notebook inside SageMaker\n",
    "# Studio. \n",
    "# LOCAL_MODE = \"SAGEMAKER_INTERNAL_IMAGE_URI\" not in os.environ\n",
    "\n",
    "LOCAL_MODE = False\n",
    "\n",
    "# This variable will be used to determine the architecture of the\n",
    "# local machine. If the machine is an ARM64 machine, you will need\n",
    "# to build a custom Docker image using the setup notebook.\n",
    "ARCHITECTURE = !(uname -m)\n",
    "\n",
    "# This is a dummy role that will be ignored when we run the\n",
    "# pipeline in Local Mode.\n",
    "DUMMY_ROLE = \"arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-11111111111111\"\n",
    "\n",
    "# We'll use these two variables to configure the steps that do not support\n",
    "# Local Mode.\n",
    "pipeline_session = PipelineSession() if not LOCAL_MODE else LocalPipelineSession(default_bucket=BUCKET)\n",
    "execution_role = os.environ[\"ROLE\"] if not LOCAL_MODE else DUMMY_ROLE\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"local\",\n",
    "        \"role\": DUMMY_ROLE,\n",
    "\n",
    "        # We need to use a custom Docker image when we run the pipeline\n",
    "        # in Local Model on an ARM64 machine.\n",
    "        \"image\": \"sagemaker-tensorflow-training-toolkit-local\" if ARCHITECTURE[0] == \"arm64\" else None,\n",
    "        \"framework_version\": None if ARCHITECTURE[0] == \"arm64\" else \"1.7-1\",\n",
    "        \"py_version\": None if ARCHITECTURE[0] == \"arm64\" else \"py38\",\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"ml.c5.xlarge\",\n",
    "        \"role\": execution_role,\n",
    "        \"image\": None,\n",
    "        \"framework_version\": \"1.7-1\",\n",
    "        \"py_version\": \"py3\",\n",
    "    }\n",
    "\n",
    "# By default, The SageMaker SDK logs events related to the default\n",
    "# configuration using the INFO level. To prevent these from spoiling\n",
    "# the output of this notebook cells, we can change the logging\n",
    "# level to ERROR instead.\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:45.690652Z",
     "start_time": "2024-04-16T16:55:42.793482400Z"
    }
   },
   "id": "ee6bc054dfdcbab",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "iam_client = boto3.client(\"iam\")\n",
    "sagemaker_client = boto3.client(\"sagemaker\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:45.918740500Z",
     "start_time": "2024-04-16T16:55:45.691652300Z"
    }
   },
   "id": "fc3c33517f84f15f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'s3://football-data-kamil/football/data/y.csv'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "df_local_path = str(os.environ['DATA_FILEPATH_X'])\n",
    "y_local_path = str(os.environ['DATA_FILEPATH_Y'])\n",
    "\n",
    "S3Uploader.upload(local_path=df_local_path, desired_s3_uri=f\"{S3_LOCATION}/data\", sagemaker_session=config['session'])\n",
    "S3Uploader.upload(local_path=y_local_path, desired_s3_uri=f\"{S3_LOCATION}/data\", sagemaker_session=config['session'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:47.933056100Z",
     "start_time": "2024-04-16T16:55:45.918740500Z"
    }
   },
   "id": "54a0a7017c5afb2f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:47.996080900Z",
     "start_time": "2024-04-16T16:55:47.922057100Z"
    }
   },
   "id": "36f2b2ff5af7f7bf",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.retry import (\n",
    "    SageMakerJobExceptionTypeEnum,\n",
    "    SageMakerJobStepRetryPolicy\n",
    ")\n",
    "\n",
    "retry_policy = {\n",
    "    \"ExceptionType\": SageMakerJobExceptionTypeEnum.INTERNAL_ERROR,\n",
    "    \"IntervalSeconds\": 2,\n",
    "    \"BackoffRate\": 2,\n",
    "    \"MaxAttempts\": 5,\n",
    "    \"ExpireAfterMin\": 5\n",
    "}\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"split-and-transform-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=config[\"role\"],\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "split_and_transform_data_step = ProcessingStep(\n",
    "    name=\"split-and-transform-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(source=dataset_location, destination=\"/opt/ml/processing/input\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "            ProcessingOutput(output_name=\"model\", source=\"/opt/ml/processing/model\"),\n",
    "            ProcessingOutput(output_name=\"baseline\", source=\"/opt/ml/processing/baseline\"),\n",
    "        ]\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:48.083685300Z",
     "start_time": "2024-04-16T16:55:47.997081700Z"
    }
   },
   "id": "9c975fc0201b8f4d",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "# session1_pipeline = Pipeline(\n",
    "#     name=\"session1-pipeline\",\n",
    "#     parameters=[dataset_location],\n",
    "#     steps=[\n",
    "#         split_and_transform_data_step,\n",
    "#     ],\n",
    "#     pipeline_definition_config=pipeline_definition_config,\n",
    "#     sagemaker_session=config['session'],\n",
    "# )\n",
    "# \n",
    "# session1_pipeline.upsert(role_arn=config[\"role\"])\n",
    "# session1_pipeline.start()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:48.193552100Z",
     "start_time": "2024-04-16T16:55:48.084685Z"
    }
   },
   "id": "98e68ac3054aacb8",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.c5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "use_spot_instances = True and not LOCAL_MODE\n",
    "max_run = 1000\n",
    "max_wait = 1200 if use_spot_instances else None\n",
    "instance_type = \"ml.m5.2xlarge\" \n",
    "\n",
    "estimator = XGBoost(\n",
    "    base_job_name=\"training\",\n",
    "    entry_point=f\"{CODE_FOLDER}/train.py\",\n",
    "    role=config['role'],\n",
    "    instance_count=1,\n",
    "    instance_type=config['instance_type'],\n",
    "    framework_version=config['framework_version'],\n",
    "    disable_profiler=True,\n",
    "    py_version=config['py_version'],\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'validation:logloss', 'Regex': '.*\\[[0-9]+\\].*#011validation_0-logloss:([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
    "        {'Name': 'validation:f1', 'Regex': 'F1 score: ([0-9\\\\.]+)'}\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:48.339740100Z",
     "start_time": "2024-04-16T16:55:48.194553Z"
    }
   },
   "id": "830a03c119740bea",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: training-2024-04-16-17-07-43-232\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Join is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 22\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msagemaker\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minputs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TrainingInput\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# train_model_step = TrainingStep(\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#     name=\"train-model\",\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#     estimator=estimator,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#     cache_config=cache_config\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[0;32m     20\u001B[0m train_model_step \u001B[38;5;241m=\u001B[39m TrainingStep(\n\u001B[0;32m     21\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain-model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m---> 22\u001B[0m     step_args\u001B[38;5;241m=\u001B[39m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mTrainingInput\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m                \u001B[49m\u001B[43ms3_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_and_transform_data_step\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproperties\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcessingOutputConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mS3Output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mS3Uri\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m                \u001B[49m\u001B[43mcontent_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext/csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     27\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mTrainingInput\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m                \u001B[49m\u001B[43ms3_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_and_transform_data_step\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproperties\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mProcessingOutputConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mS3Output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mS3Uri\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m                \u001B[49m\u001B[43mcontent_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext/csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     31\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;66;03m# cache_config=cache_config\u001B[39;00m\n\u001B[0;32m     35\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:346\u001B[0m, in \u001B[0;36mrunnable_by_pipeline.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m context\n\u001B[0;32m    344\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 346\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrun_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sagemaker\\estimator.py:1338\u001B[0m, in \u001B[0;36mEstimatorBase.fit\u001B[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001B[0m\n\u001B[0;32m   1335\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_for_training(job_name\u001B[38;5;241m=\u001B[39mjob_name)\n\u001B[0;32m   1337\u001B[0m experiment_config \u001B[38;5;241m=\u001B[39m check_and_get_run_experiment_config(experiment_config)\n\u001B[1;32m-> 1338\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatest_training_job \u001B[38;5;241m=\u001B[39m \u001B[43m_TrainingJob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_new\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexperiment_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjobs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatest_training_job)\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wait:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sagemaker\\estimator.py:2437\u001B[0m, in \u001B[0;36m_TrainingJob.start_new\u001B[1;34m(cls, estimator, inputs, experiment_config)\u001B[0m\n\u001B[0;32m   2412\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001B[39;00m\n\u001B[0;32m   2413\u001B[0m \n\u001B[0;32m   2414\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2433\u001B[0m \u001B[38;5;124;03m    all information about the started training job.\u001B[39;00m\n\u001B[0;32m   2434\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2435\u001B[0m train_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_train_args(estimator, inputs, experiment_config)\n\u001B[1;32m-> 2437\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msagemaker_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtrain_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2439\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(estimator\u001B[38;5;241m.\u001B[39msagemaker_session, estimator\u001B[38;5;241m.\u001B[39m_current_job_name)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sagemaker\\session.py:982\u001B[0m, in \u001B[0;36mSession.train\u001B[1;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, infra_check_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy, remote_debug_config)\u001B[0m\n\u001B[0;32m    979\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain request: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, json\u001B[38;5;241m.\u001B[39mdumps(request, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m))\n\u001B[0;32m    980\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msagemaker_client\u001B[38;5;241m.\u001B[39mcreate_training_job(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrequest)\n\u001B[1;32m--> 982\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_intercept_create_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_request\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubmit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sagemaker\\session.py:6427\u001B[0m, in \u001B[0;36mSession._intercept_create_request\u001B[1;34m(self, request, create, func_name)\u001B[0m\n\u001B[0;32m   6410\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_intercept_create_request\u001B[39m(\n\u001B[0;32m   6411\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   6412\u001B[0m     request: typing\u001B[38;5;241m.\u001B[39mDict,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   6415\u001B[0m     \u001B[38;5;66;03m# pylint: disable=unused-argument\u001B[39;00m\n\u001B[0;32m   6416\u001B[0m ):\n\u001B[0;32m   6417\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This function intercepts the create job request.\u001B[39;00m\n\u001B[0;32m   6418\u001B[0m \n\u001B[0;32m   6419\u001B[0m \u001B[38;5;124;03m    PipelineSession inherits this Session class and will override\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   6425\u001B[0m \u001B[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001B[39;00m\n\u001B[0;32m   6426\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 6427\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sagemaker\\session.py:979\u001B[0m, in \u001B[0;36mSession.train.<locals>.submit\u001B[1;34m(request)\u001B[0m\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msubmit\u001B[39m(request):\n\u001B[0;32m    978\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating training-job with name: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, job_name)\n\u001B[1;32m--> 979\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain request: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    980\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msagemaker_client\u001B[38;5;241m.\u001B[39mcreate_training_job(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrequest)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:238\u001B[0m, in \u001B[0;36mdumps\u001B[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONEncoder\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskipkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_ascii\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_ascii\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_circular\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_circular\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseparators\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseparators\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m--> 238\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:202\u001B[0m, in \u001B[0;36mJSONEncoder.encode\u001B[1;34m(self, o)\u001B[0m\n\u001B[0;32m    200\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterencode(o, _one_shot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m--> 202\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(chunks)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:432\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[1;34m(o, _current_indent_level)\u001B[0m\n\u001B[0;32m    430\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_list(o, _current_indent_level)\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(o, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m--> 432\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_dict(o, _current_indent_level)\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[1;34m(dct, _current_indent_level)\u001B[0m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[1;32m--> 406\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    408\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:326\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_list\u001B[1;34m(lst, _current_indent_level)\u001B[0m\n\u001B[0;32m    324\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    325\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[1;32m--> 326\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    328\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[1;34m(dct, _current_indent_level)\u001B[0m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[1;32m--> 406\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    408\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[1;34m(dct, _current_indent_level)\u001B[0m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[1;32m--> 406\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    408\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[1;34m(dct, _current_indent_level)\u001B[0m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[1;32m--> 406\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    408\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:439\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[1;34m(o, _current_indent_level)\u001B[0m\n\u001B[0;32m    437\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCircular reference detected\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    438\u001B[0m     markers[markerid] \u001B[38;5;241m=\u001B[39m o\n\u001B[1;32m--> 439\u001B[0m o \u001B[38;5;241m=\u001B[39m \u001B[43m_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m _iterencode(o, _current_indent_level)\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[1;34m(self, o)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[0;32m    162\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    178\u001B[0m \n\u001B[0;32m    179\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 180\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    181\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Object of type Join is not JSON serializable"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# train_model_step = TrainingStep(\n",
    "#     name=\"train-model\",\n",
    "#     estimator=estimator,\n",
    "#     inputs={\n",
    "#         \"train\": TrainingInput(\n",
    "#             s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\"\n",
    "#         ),\n",
    "#         \"validation\": TrainingInput(\n",
    "#             s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\"\n",
    "#         )\n",
    "#     },\n",
    "#     cache_config=cache_config\n",
    "# )\n",
    "\n",
    "train_model_step = TrainingStep(\n",
    "    name=\"train-model\",\n",
    "    step_args=estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri.to_string(),\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri.to_string(),\n",
    "                content_type=\"text/csv\"\n",
    "            )\n",
    "        }\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T17:07:43.661391400Z",
     "start_time": "2024-04-16T17:07:43.155699300Z"
    }
   },
   "id": "5dd9879e78a475aa",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "USE_TUNING_STEP = False and not LOCAL_MODE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:16.687335600Z",
     "start_time": "2024-04-16T16:55:16.583804700Z"
    }
   },
   "id": "51bc8ae0e97f4cfd",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 27\u001B[0m\n\u001B[0;32m     23\u001B[0m metric_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation:f1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     24\u001B[0m strategy \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBayesian\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     26\u001B[0m tuner \u001B[38;5;241m=\u001B[39m HyperparameterTuner(\n\u001B[1;32m---> 27\u001B[0m     estimator\u001B[38;5;241m=\u001B[39m\u001B[43mestimator\u001B[49m,\n\u001B[0;32m     28\u001B[0m     objective_metric_name\u001B[38;5;241m=\u001B[39mmetric_name,\n\u001B[0;32m     29\u001B[0m     objective_type\u001B[38;5;241m=\u001B[39mobjective_type,\n\u001B[0;32m     30\u001B[0m     hyperparameter_ranges\u001B[38;5;241m=\u001B[39mhyperparameter_ranges,\n\u001B[0;32m     31\u001B[0m     metric_definitions\u001B[38;5;241m=\u001B[39mmetric_definitions,\n\u001B[0;32m     32\u001B[0m     max_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,\n\u001B[0;32m     33\u001B[0m     max_parallel_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     34\u001B[0m     early_stopping_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAuto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     35\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'estimator' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.parameter import IntegerParameter, ContinuousParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(min_value=0.05, max_value=0.3, scaling_type=\"Logarithmic\"),\n",
    "    'max_depth': IntegerParameter(min_value=5, max_value=15, scaling_type=\"Auto\"),\n",
    "    'subsample': ContinuousParameter(min_value=0.7, max_value=1.0, scaling_type=\"Auto\"),\n",
    "    'colsample_bytree': ContinuousParameter(min_value=0.7, max_value=1.0, scaling_type=\"Logarithmic\"),\n",
    "    'lambda': ContinuousParameter(min_value=5, max_value=12, scaling_type=\"Logarithmic\"),\n",
    "    'alpha': ContinuousParameter(min_value=1, max_value=10, scaling_type=\"Logarithmic\"),\n",
    "    'min_child_weight': ContinuousParameter(min_value=0.4, max_value=1.0, scaling_type=\"Auto\"),\n",
    "}\n",
    "\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [\n",
    "    {'Name': 'validation:logloss',\n",
    "     'Regex': '.*\\[[0-9]+\\].*#011validation_0-logloss:([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'\n",
    "     },\n",
    "    {'Name': 'validation:f1',\n",
    "     'Regex': 'F1 score: ([0-9\\\\.]+)'\n",
    "     },\n",
    "]\n",
    "metric_name = \"validation:f1\"\n",
    "strategy = \"Bayesian\"\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=metric_name,\n",
    "    objective_type=objective_type,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=8,\n",
    "    max_parallel_jobs=2,\n",
    "    early_stopping_type='Auto',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T16:55:19.903586900Z",
     "start_time": "2024-04-16T16:55:19.394907800Z"
    }
   },
   "id": "a31e770437cfcbb6",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "# tune_model_step = TuningStep(\n",
    "#     name=\"tune-model\",\n",
    "#     tuner=tuner,\n",
    "#     inputs={\n",
    "#         \"train\": TrainingInput(\n",
    "#             s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\"\n",
    "#         ),\n",
    "#         \"validation\": TrainingInput(\n",
    "#             s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "#             content_type=\"text/csv\"\n",
    "#         )\n",
    "#     },\n",
    "#     cache_config=cache_config\n",
    "# )\n",
    "\n",
    "tune_model_step = TuningStep(\n",
    "    name=\"tune-model\",\n",
    "    step_args=tuner.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-16T16:04:03.037370500Z"
    }
   },
   "id": "473cacc2979587a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "session2_pipeline = Pipeline(\n",
    "    name=\"session2-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        split_and_transform_data_step,\n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "session2_pipeline.upsert(role_arn=config['role'])\n",
    "session2_pipeline.start()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-16T16:04:03.038370300Z"
    }
   },
   "id": "fdd3e81e0dca46aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.c5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost import XGBoostProcessor\n",
    "\n",
    "evaluation_processor = XGBoostProcessor(\n",
    "    base_job_name=\"evaluation-processor\",\n",
    "    image_uri=config[\"image\"],\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=config['role'],\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.440229200Z",
     "start_time": "2024-04-15T11:58:37.357145500Z"
    }
   },
   "id": "d73cf8acd2617076",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"evaluation-report\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.508080200Z",
     "start_time": "2024-04-15T11:58:37.439228900Z"
    }
   },
   "id": "f41bc50b964b336a",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "evaluate_model_step = ProcessingStep(\n",
    "    name=\"evaluate-model\",\n",
    "    step_args=evaluation_processor.run(\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=tune_model_step.get_top_model_s3_uri(\n",
    "                    top_k=0, s3_bucket=config[\"session\"].default_bucket()\n",
    "                ) if USE_TUNING_STEP else train_model_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n",
    "            ),\n",
    "        ],\n",
    "        code=f\"{CODE_FOLDER}/evaluation.py\",\n",
    "    ),\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.577042300Z",
     "start_time": "2024-04-15T11:58:37.509080500Z"
    }
   },
   "id": "2147eaf6e5f6b450",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Code to retrieve last model metric.\n",
    "# from sagemaker.workflow.parameters import ParameterFloat\n",
    "# import json\n",
    "# \n",
    "# response = sagemaker_client.list_model_packages(ModelPackageGroupName=MODEL_PACKAGE_GROUP,\n",
    "#                                                 SortBy='CreationTime',\n",
    "#                                                 SortOrder='Descending',\n",
    "#                                                 MaxResults=1)\n",
    "# \n",
    "# if response['ModelPackageSummaryList']:\n",
    "#     latest_model_package_arn = response['ModelPackageSummaryList'][0]['ModelPackageArn']\n",
    "#     model_package_description = sagemaker_client.describe_model_package(ModelPackageName=latest_model_package_arn)\n",
    "#     evaluation_metrics = model_package_description['ModelMetrics']['ModelQuality']['Statistics']\n",
    "# \n",
    "#     s3_uri = evaluation_metrics['S3Uri']\n",
    "#     s3_bucket, s3_key = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "# \n",
    "#     obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "#     evaluation_json = json.loads(obj['Body'].read())\n",
    "#     f1_metric = evaluation_json['metrics']['f1']['value']\n",
    "# \n",
    "#     f1_threshold = ParameterFloat(name=\"f1_threshold\", default_value=f1_metric)\n",
    "#     print(f\"F1 score from the latest model's evaluation: {f1_metric}\")\n",
    "# else:\n",
    "#     print(\"No models found in the specified model package group.\")\n",
    "#     f1_metric = 0.67\n",
    "#     f1_threshold = ParameterFloat(name=\"f1_threshold\", default_value=f1_metric)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.644403100Z",
     "start_time": "2024-04-15T11:58:37.578042300Z"
    }
   },
   "id": "b0d757e05ef0a9cc",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "transformation_pipeline_model = Join(\n",
    "    on=\"/\",\n",
    "    values=[\n",
    "        split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\n",
    "            \"model\"\n",
    "        ].S3Output.S3Uri,\n",
    "        \"model.tar.gz\",\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.713301600Z",
     "start_time": "2024-04-15T11:58:37.645402600Z"
    }
   },
   "id": "eeeea4494684fe0b",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoostModel\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "preprocessing_model = SKLearnModel(\n",
    "    model_data=transformation_pipeline_model,\n",
    "    entry_point=\"preprocessing_component.py\",\n",
    "    source_dir=str(INFERENCE_CODE_FOLDER),\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=config['role'],\n",
    ")\n",
    "xgb_model = XGBoostModel(\n",
    "    model_data=tune_model_step.get_top_model_s3_uri(\n",
    "        top_k=0, s3_bucket=config[\"session\"].default_bucket()\n",
    "    ) if USE_TUNING_STEP else train_model_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=config[\"role\"],\n",
    ")\n",
    "\n",
    "post_processing_model = SKLearnModel(\n",
    "    model_data=transformation_pipeline_model,\n",
    "    entry_point=\"postprocessing_component.py\",\n",
    "    source_dir=str(INFERENCE_CODE_FOLDER),\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=config['role'],\n",
    ")\n",
    "\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name=\"inference-model\",\n",
    "    models=[preprocessing_model, xgb_model, post_processing_model],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=config['role'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.782277Z",
     "start_time": "2024-04-15T11:58:37.714301100Z"
    }
   },
   "id": "e9d6f56e8ed5ed37",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.quality_check_step import DataQualityCheckConfig, QualityCheckStep\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "DATA_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/data-quality\"\n",
    "\n",
    "data_quality_baseline_step = QualityCheckStep(\n",
    "    name=\"generate-data-quality-baseline\",\n",
    "\n",
    "    check_job_config=CheckJobConfig(\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=20,\n",
    "        sagemaker_session=config['session'],\n",
    "        role=config['role'],\n",
    "    ),\n",
    "\n",
    "    quality_check_config=DataQualityCheckConfig(\n",
    "        baseline_dataset=f\"{S3_LOCATION}/data\",\n",
    "        dataset_format=DatasetFormat.csv(header=True, output_columns_position=\"END\"),\n",
    "        output_s3_uri=DATA_QUALITY_LOCATION\n",
    "    ),\n",
    "\n",
    "    skip_check=True,\n",
    "    register_new_baseline=True,\n",
    "    cache_config=cache_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:37.866476300Z",
     "start_time": "2024-04-15T11:58:37.782277Z"
    }
   },
   "id": "c986f48af212aaad",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "create_model_step = ModelStep(\n",
    "    name=\"create-model-step\",\n",
    "    display_name=\"create-model\",\n",
    "    step_args=pipeline_model.create(\n",
    "        instance_type=\"ml.m5.xlarge\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=create_model_step.properties.ModelName,\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    strategy=\"MultiRecord\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=f\"{S3_LOCATION}/transform\",\n",
    "    sagemaker_session=config[\"session\"]\n",
    ")\n",
    "\n",
    "generate_test_predictions_step = TransformStep(\n",
    "    name=\"generate-test-predictions\",\n",
    "    step_args=transformer.transform(\n",
    "        # We will use the baseline set we generated when we split the data.\n",
    "        # This set corresponds to the test split before the transformation step.\n",
    "        data=split_and_transform_data_step.properties.ProcessingOutputConfig.Outputs[\"baseline\"].S3Output.S3Uri,\n",
    "\n",
    "        join_source=\"Input\",\n",
    "        split_type=\"Line\",\n",
    "        content_type=\"text/csv\",\n",
    "\n",
    "        # We want to output the first and the last field from the joint set.\n",
    "        # The first field corresponds to the groundtruth, and the last field\n",
    "        # corresponds to the prediction.\n",
    "        output_filter=\"$[0,-1]\",\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:40.527332900Z",
     "start_time": "2024-04-15T11:58:37.867475500Z"
    }
   },
   "id": "4d24821c4c814a93",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.quality_check_step import ModelQualityCheckConfig\n",
    "\n",
    "MODEL_QUALITY_LOCATION = f\"{S3_LOCATION}/monitoring/model-quality\"\n",
    "\n",
    "model_quality_baseline_step = QualityCheckStep(\n",
    "    name=\"generate-model-quality-baseline\",\n",
    "\n",
    "    check_job_config=CheckJobConfig(\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        volume_size_in_gb=20,\n",
    "        sagemaker_session=pipeline_session,\n",
    "        role=execution_role,\n",
    "    ),\n",
    "\n",
    "    quality_check_config=ModelQualityCheckConfig(\n",
    "        # We are going to use the output of the Transform Step to generate\n",
    "        # the model quality baseline.\n",
    "        baseline_dataset=generate_test_predictions_step.properties.TransformOutput.S3OutputPath,\n",
    "        dataset_format=DatasetFormat.csv(header=False),\n",
    "\n",
    "        # We need to specify the problem type and the fields where the prediction\n",
    "        # and groundtruth are so the process knows how to interpret the results.\n",
    "        problem_type=\"BinaryClassification\",\n",
    "\n",
    "        # Since the data doesn't have headers, SageMaker will autocreate headers for it.\n",
    "        # _c0 corresponds to the first column, and _c1 corresponds to the second column.\n",
    "        ground_truth_attribute=\"_c0\",\n",
    "        inference_attribute=\"_c1\",\n",
    "\n",
    "        output_s3_uri=MODEL_QUALITY_LOCATION,\n",
    "    ),\n",
    "\n",
    "    skip_check=True,\n",
    "    register_new_baseline=True,\n",
    "    cache_config=cache_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:40.609879Z",
     "start_time": "2024-04-15T11:58:40.528334900Z"
    }
   },
   "id": "3212e573f9954582",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_data_statistics=MetricsSource(\n",
    "        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_data_constraints=MetricsSource(\n",
    "        s3_uri=data_quality_baseline_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "\n",
    "    model_constraints=MetricsSource(\n",
    "        s3_uri=model_quality_baseline_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "drift_check_baselines = DriftCheckBaselines(\n",
    "    model_data_statistics=MetricsSource(\n",
    "        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_data_constraints=MetricsSource(\n",
    "        s3_uri=data_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckStatistics,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    model_constraints=MetricsSource(\n",
    "        s3_uri=model_quality_baseline_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:40.679981Z",
     "start_time": "2024-04-15T11:58:40.612880400Z"
    }
   },
   "id": "b921457e0e94e720",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.c5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_MODEL_PACKAGE_GROUP = \"pipeline\"\n",
    "\n",
    "register_model_step = ModelStep(\n",
    "    name=\"register\",\n",
    "    display_name=\"register-model\",\n",
    "    step_args=pipeline_model.register(\n",
    "        model_package_group_name=PIPELINE_MODEL_PACKAGE_GROUP,\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"PendingManualApproval\",\n",
    "        content_types=[\"text/csv\", \"application/json\"],\n",
    "        response_types=[\"text/csv\", \"application/json\"],\n",
    "        inference_instances=[config[\"instance_type\"]],\n",
    "        transform_instances=[config[\"instance_type\"]],\n",
    "        domain=\"MACHINE_LEARNING\",\n",
    "        task=\"CLASSIFICATION\",\n",
    "        framework=\"XGBOOST\",\n",
    "        framework_version=config[\"framework_version\"],\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:41.716197300Z",
     "start_time": "2024-04-15T11:58:40.680980500Z"
    }
   },
   "id": "268885994c369aea",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.parameters import ParameterFloat\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "\n",
    "accuracy_threshold = ParameterFloat(name=\"f1_score_threshold\", default_value=0.64)\n",
    "\n",
    "condition_step = ConditionStep(\n",
    "    name=\"check-model-f1_score\",\n",
    "    conditions=[\n",
    "        ConditionGreaterThanOrEqualTo(\n",
    "            left=JsonGet(\n",
    "                step_name=evaluate_model_step.name,\n",
    "                property_file=evaluation_report,\n",
    "                json_path=\"metrics.f1.value\",\n",
    "            ),\n",
    "            right=accuracy_threshold,\n",
    "        )\n",
    "    ],\n",
    "    if_steps=[],\n",
    "\n",
    "    else_steps=[\n",
    "        FailStep(\n",
    "            name=\"fail\",\n",
    "            error_message=Join(\n",
    "                on=\" \",\n",
    "                values=[\n",
    "                    \"Execution failed because the model's accuracy was lower than\",\n",
    "                    accuracy_threshold,\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "if not LOCAL_MODE:\n",
    "    # SageMaker doesn't support running any of these steps in Local Mode.\n",
    "    condition_step.if_steps.extend(\n",
    "        [\n",
    "            create_model_step,\n",
    "            generate_test_predictions_step,\n",
    "            model_quality_baseline_step,\n",
    "            register_model_step,\n",
    "        ]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:41.787630100Z",
     "start_time": "2024-04-15T11:58:41.721200600Z"
    }
   },
   "id": "54c46d11f5898d52",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded None to s3://sagemaker-eu-north-1-284415450706/session4-pipeline/code/a01b8389f6ae7461a33ca11537e1facf/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-eu-north-1-284415450706/session4-pipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "INFO:sagemaker.processing:Uploaded None to s3://sagemaker-eu-north-1-284415450706/session4-pipeline/code/a01b8389f6ae7461a33ca11537e1facf/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-eu-north-1-284415450706/session4-pipeline/code/2c207c809cb0e0e9a1d77e5247f961f9/runproc.sh\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'PipelineArn': 'arn:aws:sagemaker:eu-north-1:284415450706:pipeline/session4-pipeline',\n 'ResponseMetadata': {'RequestId': '6720208d-fc08-48fa-9fc4-ba4a506c95cd',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '6720208d-fc08-48fa-9fc4-ba4a506c95cd',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '86',\n   'date': 'Mon, 15 Apr 2024 11:58:45 GMT'},\n  'RetryAttempts': 0}}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"session4-pipeline\",\n",
    "    parameters=[\n",
    "        dataset_location,\n",
    "        accuracy_threshold,\n",
    "    ],\n",
    "    steps=[\n",
    "        split_and_transform_data_step,\n",
    "        tune_model_step if USE_TUNING_STEP else train_model_step,\n",
    "        evaluate_model_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    pipeline_definition_config=PipelineDefinitionConfig(use_custom_job_prefix=True),\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "if not LOCAL_MODE:\n",
    "    # SageMaker doesn't support running any of these steps in Local Mode.\n",
    "    pipeline.steps.extend([data_quality_baseline_step])\n",
    "\n",
    "pipeline.upsert(role_arn=config[\"role\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:45.743645900Z",
     "start_time": "2024-04-15T11:58:41.788630Z"
    }
   },
   "id": "4fae2bbc734c6a7c",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "_PipelineExecution(arn='arn:aws:sagemaker:eu-north-1:284415450706:pipeline/session4-pipeline/execution/9jfdxhlkr9d9', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000001EE9261F860>)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.start()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:46.031719600Z",
     "start_time": "2024-04-15T11:58:45.726648400Z"
    }
   },
   "id": "1be3fab43b4a2738",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T11:58:46.032720200Z",
     "start_time": "2024-04-15T11:58:46.020188100Z"
    }
   },
   "id": "19d14e6e6687375a",
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
